In previous section, we convert each word to token which is a representing number. Now we need to map this number to a vector, and the vector is called embedding. In deep learning, every object which can't be described by tranditional data 
structure will be represent by using a vector, such as image, voice, word, audio. Initially fields in the vector are taken as random number, then we use lots data with the deep learning model to train those vector which is gradually changing
the value of fields in the vector and those fields in the vector can contains some kind of "knowledege". For example after changing the model to konw how to related an human face image with a vector, then you can send a vector to the model
and it will output an image with human face.

Let's simulate the process as following, assuming we have a vocab with size 6, that is the vocab only contain 6 words, if we want to use vector has length of 3 ,of cause you can decide the length of the vector, usally the longer the better but
you will need more data and time to train the vector. Let's see how we can initailize six random vectors for 6 tokens in the vocab by using torch as following:

```py
import torch
#random seed to generate random values for vector
torch.manual_seed(321)
vocab_size = 6 #6 tokens
output_dim = 3 #each token map to vector with length 3
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
print(embedding_layer.weights)
```

Running above code we will get the following output:

```py
Parameter containing:
tensor([[-0.1302,  0.4343, -0.4491],
        [-1.0824,  2.5830, -0.3784],
        [-0.6681, -0.4460, -0.4942],
        [-1.0153,  0.9791,  1.5577],
        [-0.3924,  0.4283,  0.6376],
        [-0.5494,  0.7509,  1.7671]], requires_grad=True)
```

We can see that the output is a matrix with 6 rows and 3 columns, therefore each row is an embedding for each token in the vocab. Here we have an object with name embedding_layer which is a special class that provide many utility functions.
The most used one is returning given row the given index such as following:

```py
input_ids = torch.tensor([2, 3, 5, 1])
print(embedding_layer(input_ids))
```
The input_ids is a tensor which contains indexes for rows that we want to get, the output of above code is :

```py
tensor([[-0.6681, -0.4460, -0.4942],
        [-1.0153,  0.9791,  1.5577],
        [-0.5494,  0.7509,  1.7671],
        [-1.0824,  2.5830, -0.3784]], grad_fn=<EmbeddingBackward0>)
```
We can see the the output above contains the 2th, 3th, 5th and 1th row from the matrix above, and we can see that the given token value is actually identical to the index for its vector in the matrix. The embedding layer is an
entry point for the deep learning model of llm. 

Besides converting token to vector, we also need to convert a kind of very important info to the llm which is the position of the token in the sentence. When we convert token to vector, we only mapping the token value to given vector.
But in a sentence, a given token may appear in many places, the position of given token is very helpful for llm to understand the logic of how token compsite to a sentence. ChatGPT use a scheme name absolute positional encoding that is
given the position of the token, the scheme will map the position to a vector with the same length of the token embedding, the mapping is somewhat like mapping from token to embedding, we will intialize a vector with random number for 
the position number and gradually change those number in the vector during the training process.

Now let's combine with the data loader and set the vector size to 256 which is more practical, First we get the vocab side for tiktoken with encoding scheme of "gpt2" as following:

```py
encoding = tiktoken.get_encoding('gpt2')
vocab_size = encoding.n_vocab
print(vocab_size)
```
The above code has following result:
50257
which means there are at most 50257 tokens for the encoding scheme of "gpt2", then we can construct embedding for each token with length of 256 as following:

```py
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
```
Then we can load a batch with 8 sentences from the raw text and set each sentence has 4 tokens, the code as following: 

```py
max_length = 4
dataloader = create_dataloader_v1(raw_text, batch_size = 8, window_size = max_length, shift = max_length, shuffle = False)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)
print(f"Token IDs: \n", inputs)
print("\nInputs shape: \n", inputs.shape)
```
The output of above code is:

```py
Token IDs: 
 tensor([[   27,     0, 18227,  4177],
        [   56, 11401, 27711,    29],
        [  198,    27,  6494,  1398],
        [ 2625, 16366,    12,  3919],
        [ 8457, 15879,    12, 30053],
        [   12, 16129,    12,   259],
        [   12, 25677,    12, 25616],
        [15879,    12, 30053,    12]])

Inputs shape: 
 torch.Size([8, 4])
```

Now we can convert those token id in the matrix above to embeddings:

```py
token_embeddings = token_embedding_layer(inputs)
print(token_embeddings.shape)
```
The above code has following result:
```py
torch.Size([8, 4, 256])
```
It is understandable that, each token id map to a vector with length 256, therefore the token matrix of shape [8,4] is change to [8,4,256]. For absolute position encoding, since the sentence has max 4 tokens, and there are 4 position of:
1, 2, 3, 4, then we can mapping these number to vectors with the same length as following:

```py
context_length = max_length 
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)
#torch.arange(context_length) generate [0, 1, 2, 3]
pos_embeddings = pos_embedding_layer(torch.arange(context_length))
print(pos_embeddings.shape)
```
The above code has the following result:

```py
torch.Size([4, 256])
```

The final embeddings send into llm is the token embedding plus their posistion embedding as following:
```py
input_embeddings = token_embeddings + pos_embeddings 
print(input_embeddings)
```
The output of above code will like following:

```py
tensor([[[ 0.4707, -0.5548,  1.6110,  ...,  2.0232,  0.1589, -1.4503],
         [ 2.7354,  0.7785,  0.2636,  ...,  0.6698, -4.6256,  0.2454],
         [-2.9696,  1.4847,  0.8292,  ...,  1.4316,  0.0567, -0.5988],
         [ 0.3262,  0.9588,  0.6520,  ..., -0.2436,  2.6629, -0.4146]],

        [[-0.2752, -2.3073, -0.1332,  ...,  0.9190, -2.2662,  1.1484],
         [ 1.8600,  0.3578,  1.3777,  ...,  2.2377, -4.1326,  2.8430],
         [-2.9329, -0.1163,  0.0754,  ...,  2.0116, -3.3202, -0.0452],
         [ 0.0766,  0.1125,  1.7159,  ...,  1.9491, -0.2940, -1.6671]],

        [[-1.1858, -1.1984,  2.1900,  ...,  0.3141, -0.2976, -0.9662],
         [ 3.1516, -0.3418,  0.9400,  ...,  2.2662, -3.0799, -0.6063],
         [-1.9556,  4.3700,  0.4663,  ...,  1.8101,  0.1262, -1.2754],
         [ 0.0307, -0.7928,  1.3775,  ...,  0.6137,  1.6004, -2.0457]],

        ...,

        [[-1.6415,  0.7224,  2.7702,  ...,  2.1515, -0.7594, -0.5652],
         [ 2.2364, -0.3365, -0.2640,  ...,  1.4591, -3.8474,  2.1876],
         [-2.0855,  2.0819,  1.6887,  ...,  2.4914, -1.6080, -1.1573],
         [-0.1590,  0.8103,  1.7577,  ..., -0.4327,  0.5777, -2.0560]],

        [[-1.6415,  0.7224,  2.7702,  ...,  2.1515, -0.7594, -0.5652],
         [-1.5592, -2.3644, -2.1059,  ...,  0.6133, -3.7497, -1.3446],
         [-2.0855,  2.0819,  1.6887,  ...,  2.4914, -1.6080, -1.1573],
         [ 0.5131, -0.1374,  2.2446,  ..., -0.7197,  0.1920, -2.2625]],

        [[-1.0378, -1.7946,  1.4663,  ..., -0.1046, -0.1507, -0.2322],
         [ 1.0394,  0.9354,  2.0993,  ...,  2.3946, -3.9982,  0.2787],
         [-2.2387,  0.3871, -0.4970,  ...,  0.1348, -0.9972, -0.9989],
         [ 0.0786,  2.2688,  3.2343,  ...,  2.6633,  0.5390, -1.1132]]],
       grad_fn=<AddBackward0>)
```

Now we have completed the section for training data preprocess, in next section we will see have to design the attention mechanism.
